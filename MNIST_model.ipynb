{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "MNIST_model.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyNcl31fUW9fUtX6Vqz8/q2O",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/harrythu25/MNSIT_with_user_input/blob/master/MNIST_model.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YB31OgKrh7Zx"
      },
      "source": [
        "# Import the relevant packages\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D4y_GBXJe2xR"
      },
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import tensorflow_datasets as tfds"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HcDMLlKLiw_w"
      },
      "source": [
        "# Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LW7eNmxtg22V"
      },
      "source": [
        "mnist_dataset, mnist_info = tfds.load(name='mnist', with_info=True, as_supervised=True, try_gcs=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vzkx6WEgg20J"
      },
      "source": [
        "mnist_train, mnist_test = mnist_dataset['train'], mnist_dataset['test']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lqXpwFn2jliX"
      },
      "source": [
        "## Split between validation and training set"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pHTOaczCg2xt"
      },
      "source": [
        "\n",
        "\n",
        "num_validation_samples = 0.1 * mnist_info.splits['train'].num_examples #0.1 because only want 10%\n",
        "num_validation_samples = tf.cast(num_validation_samples, tf.int64) #cast to integer\n",
        "\n",
        "num_test_samples = mnist_info.splits['test'].num_examples\n",
        "num_test_samples = tf.cast(num_test_samples, tf.int64) #cast to integer\n",
        "\n",
        "#preparing the data sets before splitting\n",
        "\n",
        "#take input and transform\n",
        "def scale (image, label):\n",
        "  image = tf.cast (image, tf.float32) #make sure it's float\n",
        "  image /= 255. #values are between 0 and 255 based on the shades; divide everything by 255 so each input is between 0 and 1  \n",
        "  return image, label\n",
        "\n",
        "\n",
        "#scale the data\n",
        "scaled_train_and_validation_data = mnist_train.map(scale) #use function to transform train data\n",
        "test_data = mnist_test.map(scale) #use function to transform test data\n",
        "\n",
        "#shuffle data so batches wont affect model\n",
        "BUFFER_SIZE = 10000 \n",
        "\n",
        "shuffled_train_and_validation_data = scaled_train_and_validation_data.shuffle(BUFFER_SIZE)\n",
        "\n",
        "validation_data = shuffled_train_and_validation_data.take(num_validation_samples)\n",
        "train_data = shuffled_train_and_validation_data.skip(num_validation_samples)\n",
        "\n",
        "#Because I am going to use the mini batch gradient descent, I need to set the batch size\n",
        "#setting batch size to prepare data for model\n",
        "\n",
        "BATCH_SIZE = 100\n",
        "\n",
        "train_data = train_data.batch(BATCH_SIZE) #override .batch method with our size\n",
        "validation_data = validation_data.batch(num_validation_samples)  #to take in the whole data set, \n",
        "test_data = test_data.batch(num_test_samples)\n",
        "\n",
        "\n",
        "validation_inputs, validation_targets = next(iter(validation_data))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AD2tnnX2poJK"
      },
      "source": [
        "# Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZUhUIh6YpsZp"
      },
      "source": [
        "## Outline the Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uUWtVVR3g2p9"
      },
      "source": [
        "input_size = 784 # 28px x 28px\n",
        "output_size = 10 #0-9\n",
        "hidden_layer_size = 100\n",
        "\n",
        "model = tf.keras.Sequential([\n",
        "                             tf.keras.layers.Flatten(input_shape = (28, 28, 1)),\n",
        "                             tf.keras.layers.Dense(hidden_layer_size, activation= 'relu'), # first hidden layer\n",
        "                             tf.keras.layers.Dense(hidden_layer_size, activation= 'relu'),  # second hidden layer\n",
        "                             tf.keras.layers.Dense(output_size, activation='softmax') #output layer softmax to transform into probability since this is a classifier model\n",
        "                            ])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r9tR9wBJsCMZ"
      },
      "source": [
        "## Choose the optimizer and the loss function"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GoInY-Adg2i-"
      },
      "source": [
        "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics= ['accuracy'])\n",
        "#this aplies one hot encoding which I did not do\n",
        "#output and target layer nees to have same shape of one hot encoded format\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1b_pDdh_sziH"
      },
      "source": [
        "## Training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nOriFd3ZsyXc",
        "outputId": "409073bb-c4b1-4aa3-84aa-df093bb05705"
      },
      "source": [
        "NUM_EPOCHS =5\n",
        "\n",
        "model.fit(train_data, epochs=NUM_EPOCHS, validation_data=(validation_inputs, validation_targets), verbose =2 )"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/5\n",
            "540/540 - 8s - loss: 0.3300 - accuracy: 0.9059 - val_loss: 0.1647 - val_accuracy: 0.9498\n",
            "Epoch 2/5\n",
            "540/540 - 4s - loss: 0.1415 - accuracy: 0.9581 - val_loss: 0.1181 - val_accuracy: 0.9653\n",
            "Epoch 3/5\n",
            "540/540 - 4s - loss: 0.0985 - accuracy: 0.9709 - val_loss: 0.0814 - val_accuracy: 0.9760\n",
            "Epoch 4/5\n",
            "540/540 - 4s - loss: 0.0741 - accuracy: 0.9779 - val_loss: 0.0739 - val_accuracy: 0.9782\n",
            "Epoch 5/5\n",
            "540/540 - 4s - loss: 0.0578 - accuracy: 0.9823 - val_loss: 0.0624 - val_accuracy: 0.9797\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7fa3fedfe310>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dLefG6tFu-3u"
      },
      "source": [
        "# Test the model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8HH-qK95tkVr",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0e92c534-358c-4f21-e22f-07877c5fca04"
      },
      "source": [
        "test_loss, test_accuracy = model.evaluate(test_data)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1/1 [==============================] - 1s 1s/step - loss: 0.0817 - accuracy: 0.9747\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4O2AosXNvpgN",
        "outputId": "831e481b-55f1-470d-a2d5-4d8c86507207"
      },
      "source": [
        "print('Test loss: {0: .2}. Test accuracy: {1: .2f}%'.format(test_loss, test_accuracy*100.))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Test loss:  0.082. Test accuracy:  97.47%\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NagCA4xO0kfy"
      },
      "source": [
        "## Implementation of Model \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RQK_f7GJoDHq"
      },
      "source": [
        "I used pygame and a 28x 28 grid to allow the user to draw their number and implemented this model to predict it. Pygame does not work with online notebooks so go to my github repo for the full project\n",
        "\n",
        "\n",
        "https://github.com/harrythu25/MNSIT_with_user_input"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vZGo1MGDoRih"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}